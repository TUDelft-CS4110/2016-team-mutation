<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../.resources/report.css" type="text/css"/><link rel="shortcut icon" href="../.resources/report.gif" type="image/gif"/><title>WebCrawler.java</title><link rel="stylesheet" href="../.resources/prettify.css" type="text/css"/><script type="text/javascript" src="../.resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../.sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">crawler4j</a> &gt; <a href="index.source.html" class="el_package">edu.uci.ics.crawler4j.crawler</a> &gt; <span class="el_source">WebCrawler.java</span></div><h1>WebCrawler.java</h1><pre class="source lang-java linenums">/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the &quot;License&quot;); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package edu.uci.ics.crawler4j.crawler;

import java.util.ArrayList;
import java.util.List;
import java.util.Locale;

import org.apache.http.HttpStatus;
import org.apache.http.impl.EnglishReasonPhraseCatalog;

import edu.uci.ics.crawler4j.crawler.exceptions.ContentFetchException;
import edu.uci.ics.crawler4j.crawler.exceptions.PageBiggerThanMaxSizeException;
import edu.uci.ics.crawler4j.crawler.exceptions.ParseException;
import edu.uci.ics.crawler4j.crawler.exceptions.RedirectException;
import edu.uci.ics.crawler4j.fetcher.PageFetchResult;
import edu.uci.ics.crawler4j.fetcher.PageFetcher;
import edu.uci.ics.crawler4j.frontier.DocIDServer;
import edu.uci.ics.crawler4j.frontier.Frontier;
import edu.uci.ics.crawler4j.parser.NotAllowedContentException;
import edu.uci.ics.crawler4j.parser.ParseData;
import edu.uci.ics.crawler4j.parser.Parser;
import edu.uci.ics.crawler4j.robotstxt.RobotstxtServer;
import edu.uci.ics.crawler4j.url.WebURL;
import uk.org.lidalia.slf4jext.Level;
import uk.org.lidalia.slf4jext.Logger;
import uk.org.lidalia.slf4jext.LoggerFactory;

/**
 * WebCrawler class in the Runnable class that is executed by each crawler thread.
 *
 * @author Yasser Ganjisaffar
 */
<span class="nc" id="L49">public class WebCrawler implements Runnable {</span>

<span class="nc" id="L51">  protected static final Logger logger = LoggerFactory.getLogger(WebCrawler.class);</span>

  /**
   * The id associated to the crawler thread running this instance
   */
  protected int myId;

  /**
   * The controller instance that has created this crawler thread. This
   * reference to the controller can be used for getting configurations of the
   * current crawl or adding new seeds during runtime.
   */
  protected CrawlController myController;

  /**
   * The thread within which this crawler instance is running.
   */
  private Thread myThread;

  /**
   * The parser that is used by this crawler instance to parse the content of the fetched pages.
   */
  private Parser parser;

  /**
   * The fetcher that is used by this crawler instance to fetch the content of pages from the web.
   */
  private PageFetcher pageFetcher;

  /**
   * The RobotstxtServer instance that is used by this crawler instance to
   * determine whether the crawler is allowed to crawl the content of each page.
   */
  private RobotstxtServer robotstxtServer;

  /**
   * The DocIDServer that is used by this crawler instance to map each URL to a unique docid.
   */
  private DocIDServer docIdServer;

  /**
   * The Frontier object that manages the crawl queue.
   */
  private Frontier frontier;

  /**
   * Is the current crawler instance waiting for new URLs? This field is
   * mainly used by the controller to detect whether all of the crawler
   * instances are waiting for new URLs and therefore there is no more work
   * and crawling can be stopped.
   */
  private boolean isWaitingForNewURLs;

  /**
   * Initializes the current instance of the crawler
   *
   * @param id
   *            the id of this crawler instance
   * @param crawlController
   *            the controller that manages this crawling session
   */
  public void init(int id, CrawlController crawlController) {
<span class="nc" id="L113">    this.myId = id;</span>
<span class="nc" id="L114">    this.pageFetcher = crawlController.getPageFetcher();</span>
<span class="nc" id="L115">    this.robotstxtServer = crawlController.getRobotstxtServer();</span>
<span class="nc" id="L116">    this.docIdServer = crawlController.getDocIdServer();</span>
<span class="nc" id="L117">    this.frontier = crawlController.getFrontier();</span>
<span class="nc" id="L118">    this.parser = new Parser(crawlController.getConfig());</span>
<span class="nc" id="L119">    this.myController = crawlController;</span>
<span class="nc" id="L120">    this.isWaitingForNewURLs = false;</span>
<span class="nc" id="L121">  }</span>

  /**
   * Get the id of the current crawler instance
   *
   * @return the id of the current crawler instance
   */
  public int getMyId() {
<span class="nc" id="L129">    return myId;</span>
  }

  public CrawlController getMyController() {
<span class="nc" id="L133">    return myController;</span>
  }

  /**
   * This function is called just before starting the crawl by this crawler
   * instance. It can be used for setting up the data structures or
   * initializations needed by this crawler instance.
   */
  public void onStart() {
    // Do nothing by default
    // Sub-classed can override this to add their custom functionality
<span class="nc" id="L144">  }</span>

  /**
   * This function is called just before the termination of the current
   * crawler instance. It can be used for persisting in-memory data or other
   * finalization tasks.
   */
  public void onBeforeExit() {
    // Do nothing by default
    // Sub-classed can override this to add their custom functionality
<span class="nc" id="L154">  }</span>

  /**
   * This function is called once the header of a page is fetched. It can be
   * overridden by sub-classes to perform custom logic for different status
   * codes. For example, 404 pages can be logged, etc.
   *
   * @param webUrl WebUrl containing the statusCode
   * @param statusCode Html Status Code number
   * @param statusDescription Html Status COde description
   */
  protected void handlePageStatusCode(WebURL webUrl, int statusCode, String statusDescription) {
    // Do nothing by default
    // Sub-classed can override this to add their custom functionality
<span class="nc" id="L168">  }</span>

  /**
   * This function is called before processing of the page's URL
   * It can be overridden by subclasses for tweaking of the url before processing it.
   * For example, http://abc.com/def?a=123 - http://abc.com/def
   *
   * @param curURL current URL which can be tweaked before processing
   * @return tweaked WebURL
   */
  protected WebURL handleUrlBeforeProcess(WebURL curURL) {
<span class="nc" id="L179">    return curURL;</span>
  }

  /**
   * This function is called if the content of a url is bigger than allowed size.
   *
   * @param urlStr - The URL which it's content is bigger than allowed size
   */
  protected void onPageBiggerThanMaxSize(String urlStr, long pageSize) {
<span class="nc" id="L188">    logger.warn(&quot;Skipping a URL: {} which was bigger ( {} ) than max allowed size&quot;, urlStr, pageSize);</span>
<span class="nc" id="L189">  }</span>

  /**
   * This function is called if the crawler encountered an unexpected http status code ( a status code other than 3xx)
   *
   * @param urlStr URL in which an unexpected error was encountered while crawling
   * @param statusCode Html StatusCode
   * @param contentType Type of Content
   * @param description Error Description
   */
  protected void onUnexpectedStatusCode(String urlStr, int statusCode, String contentType, String description) {
<span class="nc" id="L200">    logger.warn(&quot;Skipping URL: {}, StatusCode: {}, {}, {}&quot;, urlStr, statusCode, contentType, description);</span>
    // Do nothing by default (except basic logging)
    // Sub-classed can override this to add their custom functionality
<span class="nc" id="L203">  }</span>

  /**
   * This function is called if the content of a url could not be fetched.
   *
   * @param webUrl URL which content failed to be fetched
   */
  protected void onContentFetchError(WebURL webUrl) {
<span class="nc" id="L211">    logger.warn(&quot;Can't fetch content of: {}&quot;, webUrl.getURL());</span>
    // Do nothing by default (except basic logging)
    // Sub-classed can override this to add their custom functionality
<span class="nc" id="L214">  }</span>
  
  /**
   * This function is called when a unhandled exception was encountered during fetching
   *
   * @param webUrl URL where a unhandled exception occured
   */
  protected void onUnhandledException(WebURL webUrl, Throwable e) {
<span class="nc bnc" id="L222" title="All 2 branches missed.">    String urlStr = (webUrl == null ? &quot;NULL&quot; : webUrl.getURL());</span>
<span class="nc" id="L223">    logger.warn(&quot;Unhandled exception while fetching {}: {}&quot;, urlStr, e.getMessage());</span>
<span class="nc" id="L224">    logger.info(&quot;Stacktrace: &quot;, e);</span>
    // Do nothing by default (except basic logging)
    // Sub-classed can override this to add their custom functionality
<span class="nc" id="L227">  }</span>
  
  /**
   * This function is called if there has been an error in parsing the content.
   *
   * @param webUrl URL which failed on parsing
   */
  protected void onParseError(WebURL webUrl) {
<span class="nc" id="L235">    logger.warn(&quot;Parsing error of: {}&quot;, webUrl.getURL());</span>
    // Do nothing by default (Except logging)
    // Sub-classed can override this to add their custom functionality
<span class="nc" id="L238">  }</span>

  /**
   * The CrawlController instance that has created this crawler instance will
   * call this function just before terminating this crawler thread. Classes
   * that extend WebCrawler can override this function to pass their local
   * data to their controller. The controller then puts these local data in a
   * List that can then be used for processing the local data of crawlers (if needed).
   *
   * @return currently NULL
   */
  public Object getMyLocalData() {
<span class="nc" id="L250">    return null;</span>
  }

  @Override
  public void run() {
<span class="nc" id="L255">    onStart();</span>
    while (true) {
<span class="nc" id="L257">      List&lt;WebURL&gt; assignedURLs = new ArrayList&lt;&gt;(50);</span>
<span class="nc" id="L258">      isWaitingForNewURLs = true;</span>
<span class="nc" id="L259">      frontier.getNextURLs(50, assignedURLs);</span>
<span class="nc" id="L260">      isWaitingForNewURLs = false;</span>
<span class="nc bnc" id="L261" title="All 2 branches missed.">      if (assignedURLs.isEmpty()) {</span>
<span class="nc bnc" id="L262" title="All 2 branches missed.">        if (frontier.isFinished()) {</span>
<span class="nc" id="L263">          return;</span>
        }
        try {
<span class="nc" id="L266">          Thread.sleep(3000);</span>
<span class="nc" id="L267">        } catch (InterruptedException e) {</span>
<span class="nc" id="L268">          logger.error(&quot;Error occurred&quot;, e);</span>
<span class="nc" id="L269">        }</span>
      } else {
<span class="nc bnc" id="L271" title="All 2 branches missed.">        for (WebURL curURL : assignedURLs) {</span>
<span class="nc bnc" id="L272" title="All 2 branches missed.">          if (myController.isShuttingDown()) {</span>
<span class="nc" id="L273">            logger.info(&quot;Exiting because of controller shutdown.&quot;);</span>
<span class="nc" id="L274">            return;</span>
          }
<span class="nc bnc" id="L276" title="All 2 branches missed.">          if (curURL != null) {</span>
<span class="nc" id="L277">            curURL = handleUrlBeforeProcess(curURL);</span>
<span class="nc" id="L278">            processPage(curURL);</span>
<span class="nc" id="L279">            frontier.setProcessed(curURL);</span>
          }
<span class="nc" id="L281">        }</span>
      }
<span class="nc" id="L283">    }</span>
  }

  /**
   * Classes that extends WebCrawler should overwrite this function to tell the
   * crawler whether the given url should be crawled or not. The following
   * default implementation indicates that all urls should be included in the crawl.
   *
   * @param url
   *            the url which we are interested to know whether it should be
   *            included in the crawl or not.
   * @param referringPage
   *           The Page in which this url was found.
   * @return if the url should be included in the crawl it returns true,
   *         otherwise false is returned.
   */
  public boolean shouldVisit(Page referringPage, WebURL url) {
    // By default allow all urls to be crawled.
<span class="nc" id="L301">    return true;</span>
  }

  /**
   * Classes that extends WebCrawler should overwrite this function to process
   * the content of the fetched and parsed page.
   *
   * @param page
   *            the page object that is just fetched and parsed.
   */
  public void visit(Page page) {
    // Do nothing by default
    // Sub-classed should override this to add their custom functionality
<span class="nc" id="L314">  }</span>

  private void processPage(WebURL curURL) {
<span class="nc" id="L317">    PageFetchResult fetchResult = null;</span>
    try {
<span class="nc bnc" id="L319" title="All 2 branches missed.">      if (curURL == null) {</span>
<span class="nc" id="L320">        throw new Exception(&quot;Failed processing a NULL url !?&quot;);</span>
      }

<span class="nc" id="L323">      fetchResult = pageFetcher.fetchPage(curURL);</span>
<span class="nc" id="L324">      int statusCode = fetchResult.getStatusCode();</span>
<span class="nc" id="L325">      handlePageStatusCode(curURL, statusCode, EnglishReasonPhraseCatalog.INSTANCE</span>
          .getReason(statusCode, Locale.ENGLISH)); // Finds the status reason for all known statuses

<span class="nc" id="L328">      Page page = new Page(curURL);</span>
<span class="nc" id="L329">      page.setFetchResponseHeaders(fetchResult.getResponseHeaders());</span>
<span class="nc" id="L330">      page.setStatusCode(statusCode);</span>
<span class="nc bnc" id="L331" title="All 4 branches missed.">      if (statusCode &lt; 200 || statusCode &gt; 299) { // Not 2XX: 2XX status codes indicate success</span>
<span class="nc bnc" id="L332" title="All 12 branches missed.">        if (statusCode == HttpStatus.SC_MOVED_PERMANENTLY || statusCode == HttpStatus.SC_MOVED_TEMPORARILY ||</span>
            statusCode == HttpStatus.SC_MULTIPLE_CHOICES || statusCode == HttpStatus.SC_SEE_OTHER ||
            statusCode == HttpStatus.SC_TEMPORARY_REDIRECT ||
            statusCode == 308) { // is 3xx  todo follow https://issues.apache.org/jira/browse/HTTPCORE-389

<span class="nc" id="L337">          page.setRedirect(true);</span>
<span class="nc bnc" id="L338" title="All 2 branches missed.">          if (myController.getConfig().isFollowRedirects()) {</span>
<span class="nc" id="L339">            String movedToUrl = fetchResult.getMovedToUrl();</span>
<span class="nc bnc" id="L340" title="All 2 branches missed.">            if (movedToUrl == null) {</span>
<span class="nc" id="L341">              throw new RedirectException(Level.WARN, &quot;Unexpected error, URL: &quot; + curURL + &quot; is redirected to NOTHING&quot;);</span>
            }
<span class="nc" id="L343">            page.setRedirectedToUrl(movedToUrl);</span>

<span class="nc" id="L345">            int newDocId = docIdServer.getDocId(movedToUrl);</span>
<span class="nc bnc" id="L346" title="All 2 branches missed.">            if (newDocId &gt; 0) {</span>
<span class="nc" id="L347">              throw new RedirectException(Level.DEBUG, &quot;Redirect page: &quot; + curURL + &quot; is already seen&quot;);</span>
            }

<span class="nc" id="L350">            WebURL webURL = new WebURL();</span>
<span class="nc" id="L351">            webURL.setURL(movedToUrl);</span>
<span class="nc" id="L352">            webURL.setParentDocid(curURL.getParentDocid());</span>
<span class="nc" id="L353">            webURL.setParentUrl(curURL.getParentUrl());</span>
<span class="nc" id="L354">            webURL.setDepth(curURL.getDepth());</span>
<span class="nc" id="L355">            webURL.setDocid(-1);</span>
<span class="nc" id="L356">            webURL.setAnchor(curURL.getAnchor());</span>
<span class="nc bnc" id="L357" title="All 2 branches missed.">            if (shouldVisit(page, webURL)) {</span>
<span class="nc bnc" id="L358" title="All 2 branches missed.">              if (robotstxtServer.allows(webURL)) {</span>
<span class="nc" id="L359">                webURL.setDocid(docIdServer.getNewDocID(movedToUrl));</span>
<span class="nc" id="L360">                frontier.schedule(webURL);</span>
              } else {
<span class="nc" id="L362">                logger.debug(&quot;Not visiting: {} as per the server's \&quot;robots.txt\&quot; policy&quot;, webURL.getURL());</span>
              }
            } else {
<span class="nc" id="L365">              logger.debug(&quot;Not visiting: {} as per your \&quot;shouldVisit\&quot; policy&quot;, webURL.getURL());</span>
            }
<span class="nc" id="L367">          }</span>
        } else { // All other http codes other than 3xx &amp; 200
<span class="nc" id="L369">          String description = EnglishReasonPhraseCatalog.INSTANCE</span>
              .getReason(fetchResult.getStatusCode(), Locale.ENGLISH); // Finds the status reason for all known statuses
<span class="nc bnc" id="L371" title="All 2 branches missed.">          String contentType =</span>
              fetchResult.getEntity() == null ? &quot;&quot; : fetchResult.getEntity().getContentType().getValue();
<span class="nc" id="L373">          onUnexpectedStatusCode(curURL.getURL(), fetchResult.getStatusCode(), contentType, description);</span>
<span class="nc" id="L374">        }</span>

      } else { // if status code is 200
<span class="nc bnc" id="L377" title="All 2 branches missed.">        if (!curURL.getURL().equals(fetchResult.getFetchedUrl())) {</span>
<span class="nc bnc" id="L378" title="All 2 branches missed.">          if (docIdServer.isSeenBefore(fetchResult.getFetchedUrl())) {</span>
<span class="nc" id="L379">            throw new RedirectException(Level.DEBUG, &quot;Redirect page: &quot; + curURL + &quot; has already been seen&quot;);</span>
          }
<span class="nc" id="L381">          curURL.setURL(fetchResult.getFetchedUrl());</span>
<span class="nc" id="L382">          curURL.setDocid(docIdServer.getNewDocID(fetchResult.getFetchedUrl()));</span>
        }

<span class="nc bnc" id="L385" title="All 2 branches missed.">        if (!fetchResult.fetchContent(page)) {</span>
<span class="nc" id="L386">          throw new ContentFetchException();</span>
        }

<span class="nc" id="L389">        parser.parse(page, curURL.getURL());</span>

<span class="nc" id="L391">        ParseData parseData = page.getParseData();</span>
<span class="nc" id="L392">        List&lt;WebURL&gt; toSchedule = new ArrayList&lt;&gt;();</span>
<span class="nc" id="L393">        int maxCrawlDepth = myController.getConfig().getMaxDepthOfCrawling();</span>
<span class="nc bnc" id="L394" title="All 2 branches missed.">        for (WebURL webURL : parseData.getOutgoingUrls()) {</span>
<span class="nc" id="L395">          webURL.setParentDocid(curURL.getDocid());</span>
<span class="nc" id="L396">          webURL.setParentUrl(curURL.getURL());</span>
<span class="nc" id="L397">          int newdocid = docIdServer.getDocId(webURL.getURL());</span>
<span class="nc bnc" id="L398" title="All 2 branches missed.">          if (newdocid &gt; 0) {</span>
            // This is not the first time that this Url is visited. So, we set the depth to a negative number.
<span class="nc" id="L400">            webURL.setDepth((short) -1);</span>
<span class="nc" id="L401">            webURL.setDocid(newdocid);</span>
          } else {
<span class="nc" id="L403">            webURL.setDocid(-1);</span>
<span class="nc" id="L404">            webURL.setDepth((short) (curURL.getDepth() + 1));</span>
<span class="nc bnc" id="L405" title="All 4 branches missed.">            if ((maxCrawlDepth == -1) || (curURL.getDepth() &lt; maxCrawlDepth)) {</span>
<span class="nc bnc" id="L406" title="All 2 branches missed.">              if (shouldVisit(page, webURL)) {</span>
<span class="nc bnc" id="L407" title="All 2 branches missed.">                if (robotstxtServer.allows(webURL)) {</span>
<span class="nc" id="L408">                  webURL.setDocid(docIdServer.getNewDocID(webURL.getURL()));</span>
<span class="nc" id="L409">                  toSchedule.add(webURL);</span>
                } else {
<span class="nc" id="L411">                  logger.debug(&quot;Not visiting: {} as per the server's \&quot;robots.txt\&quot; policy&quot;, webURL.getURL());</span>
                }
              } else {
<span class="nc" id="L414">                logger.debug(&quot;Not visiting: {} as per your \&quot;shouldVisit\&quot; policy&quot;, webURL.getURL());</span>
              }
            }
          }
<span class="nc" id="L418">        }</span>
<span class="nc" id="L419">        frontier.scheduleAll(toSchedule);</span>

<span class="nc" id="L421">        visit(page);</span>
      }
<span class="nc" id="L423">    } catch (PageBiggerThanMaxSizeException e) {</span>
<span class="nc" id="L424">      onPageBiggerThanMaxSize(curURL.getURL(), e.getPageSize());</span>
<span class="nc" id="L425">    } catch (ParseException pe) {</span>
<span class="nc" id="L426">      onParseError(curURL);</span>
<span class="nc" id="L427">    } catch (ContentFetchException cfe) {</span>
<span class="nc" id="L428">      onContentFetchError(curURL);</span>
<span class="nc" id="L429">    } catch (RedirectException re) {</span>
<span class="nc" id="L430">      logger.log(re.level, re.getMessage());</span>
<span class="nc" id="L431">    } catch (NotAllowedContentException nace) {</span>
<span class="nc" id="L432">      logger.debug(&quot;Skipping: {} as it contains binary content which you configured not to crawl&quot;, curURL.getURL());</span>
<span class="nc" id="L433">    } catch (Exception e) {</span>
<span class="nc" id="L434">      onUnhandledException(curURL, e);</span>
    } finally {
<span class="nc bnc" id="L436" title="All 16 branches missed.">      if (fetchResult != null) {</span>
<span class="nc" id="L437">        fetchResult.discardContentIfNotConsumed();</span>
      }
    }
<span class="nc" id="L440">  }</span>

  public Thread getThread() {
<span class="nc" id="L443">    return myThread;</span>
  }

  public void setThread(Thread myThread) {
<span class="nc" id="L447">    this.myThread = myThread;</span>
<span class="nc" id="L448">  }</span>

  public boolean isNotWaitingForNewURLs() {
<span class="nc bnc" id="L451" title="All 2 branches missed.">    return !isWaitingForNewURLs;</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.eclemma.org/jacoco">JaCoCo</a> 0.7.6.201602180812</span></div></body></html>