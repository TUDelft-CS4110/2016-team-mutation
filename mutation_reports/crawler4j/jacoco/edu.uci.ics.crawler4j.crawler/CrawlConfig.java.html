<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../.resources/report.css" type="text/css"/><link rel="shortcut icon" href="../.resources/report.gif" type="image/gif"/><title>CrawlConfig.java</title><link rel="stylesheet" href="../.resources/prettify.css" type="text/css"/><script type="text/javascript" src="../.resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../.sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">crawler4j</a> &gt; <a href="index.source.html" class="el_package">edu.uci.ics.crawler4j.crawler</a> &gt; <span class="el_source">CrawlConfig.java</span></div><h1>CrawlConfig.java</h1><pre class="source lang-java linenums">/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the &quot;License&quot;); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package edu.uci.ics.crawler4j.crawler;

import java.util.ArrayList;
import java.util.Collection;
import java.util.HashSet;
import java.util.List;

import org.apache.http.Header;
import org.apache.http.message.BasicHeader;

import edu.uci.ics.crawler4j.crawler.authentication.AuthInfo;

<span class="nc" id="L30">public class CrawlConfig {</span>

  /**
   * The folder which will be used by crawler for storing the intermediate
   * crawl data. The content of this folder should not be modified manually.
   */
  private String crawlStorageFolder;

  /**
   * If this feature is enabled, you would be able to resume a previously
   * stopped/crashed crawl. However, it makes crawling slightly slower
   */
<span class="nc" id="L42">  private boolean resumableCrawling = false;</span>

  /**
   * Maximum depth of crawling For unlimited depth this parameter should be
   * set to -1
   */
<span class="nc" id="L48">  private int maxDepthOfCrawling = -1;</span>

  /**
   * Maximum number of pages to fetch For unlimited number of pages, this
   * parameter should be set to -1
   */
<span class="nc" id="L54">  private int maxPagesToFetch = -1;</span>

  /**
   * user-agent string that is used for representing your crawler to web
   * servers. See http://en.wikipedia.org/wiki/User_agent for more details
   */
<span class="nc" id="L60">  private String userAgentString = &quot;crawler4j (https://github.com/yasserg/crawler4j/)&quot;;</span>

  /**
   * Default request header values.
   */
<span class="nc" id="L65">  private Collection&lt;BasicHeader&gt; defaultHeaders = new HashSet&lt;BasicHeader&gt;();</span>

  /**
   * Politeness delay in milliseconds (delay between sending two requests to
   * the same host).
   */
<span class="nc" id="L71">  private int politenessDelay = 200;</span>

  /**
   * Should we also crawl https pages?
   */
<span class="nc" id="L76">  private boolean includeHttpsPages = true;</span>

  /**
   * Should we fetch binary content such as images, audio, ...?
   */
<span class="nc" id="L81">  private boolean includeBinaryContentInCrawling = false;</span>
  
  /**
   * Should we process binary content such as image, audio, ... using TIKA?
   */
<span class="nc" id="L86">  private boolean processBinaryContentInCrawling = false;</span>

  /**
   * Maximum Connections per host
   */
<span class="nc" id="L91">  private int maxConnectionsPerHost = 100;</span>

  /**
   * Maximum total connections
   */
<span class="nc" id="L96">  private int maxTotalConnections = 100;</span>

  /**
   * Socket timeout in milliseconds
   */
<span class="nc" id="L101">  private int socketTimeout = 20000;</span>

  /**
   * Connection timeout in milliseconds
   */
<span class="nc" id="L106">  private int connectionTimeout = 30000;</span>

  /**
   * Max number of outgoing links which are processed from a page
   */
<span class="nc" id="L111">  private int maxOutgoingLinksToFollow = 5000;</span>

  /**
   * Max allowed size of a page. Pages larger than this size will not be
   * fetched.
   */
<span class="nc" id="L117">  private int maxDownloadSize = 1048576;</span>

  /**
   * Should we follow redirects?
   */
<span class="nc" id="L122">  private boolean followRedirects = true;</span>

  /**
   * Should the TLD list be updated automatically on each run? Alternatively,
   * it can be loaded from the embedded tld-names.zip file that was obtained from
   * https://publicsuffix.org/list/effective_tld_names.dat
   */
<span class="nc" id="L129">  private boolean onlineTldListUpdate = false;</span>

  /**
   * Should the crawler stop running when the queue is empty?
   */
<span class="nc" id="L134">  private boolean shutdownOnEmptyQueue = true;</span>
  
  /**
   * If crawler should run behind a proxy, this parameter can be used for
   * specifying the proxy host.
   */
<span class="nc" id="L140">  private String proxyHost = null;</span>

  /**
   * If crawler should run behind a proxy, this parameter can be used for
   * specifying the proxy port.
   */
<span class="nc" id="L146">  private int proxyPort = 80;</span>

  /**
   * If crawler should run behind a proxy and user/pass is needed for
   * authentication in proxy, this parameter can be used for specifying the
   * username.
   */
<span class="nc" id="L153">  private String proxyUsername = null;</span>

  /**
   * If crawler should run behind a proxy and user/pass is needed for
   * authentication in proxy, this parameter can be used for specifying the
   * password.
   */
<span class="nc" id="L160">  private String proxyPassword = null;</span>

  /**
   * List of possible authentications needed by crawler
   */
  private List&lt;AuthInfo&gt; authInfos;

  /**
   * Validates the configs specified by this instance.
   *
   * @throws Exception on Validation fail
   */
  public void validate() throws Exception {
<span class="nc bnc" id="L173" title="All 2 branches missed.">    if (crawlStorageFolder == null) {</span>
<span class="nc" id="L174">      throw new Exception(&quot;Crawl storage folder is not set in the CrawlConfig.&quot;);</span>
    }
<span class="nc bnc" id="L176" title="All 2 branches missed.">    if (politenessDelay &lt; 0) {</span>
<span class="nc" id="L177">      throw new Exception(&quot;Invalid value for politeness delay: &quot; + politenessDelay);</span>
    }
<span class="nc bnc" id="L179" title="All 2 branches missed.">    if (maxDepthOfCrawling &lt; -1) {</span>
<span class="nc" id="L180">      throw new Exception(&quot;Maximum crawl depth should be either a positive number or -1 for unlimited depth.&quot;);</span>
    }
<span class="nc bnc" id="L182" title="All 2 branches missed.">    if (maxDepthOfCrawling &gt; Short.MAX_VALUE) {</span>
<span class="nc" id="L183">      throw new Exception(&quot;Maximum value for crawl depth is &quot; + Short.MAX_VALUE);</span>
    }
<span class="nc" id="L185">  }</span>

  public String getCrawlStorageFolder() {
<span class="nc" id="L188">    return crawlStorageFolder;</span>
  }

  /**
   * The folder which will be used by crawler for storing the intermediate
   * crawl data. The content of this folder should not be modified manually.
   *
   * @param crawlStorageFolder The folder for the crawler's storage
   */
  public void setCrawlStorageFolder(String crawlStorageFolder) {
<span class="nc" id="L198">    this.crawlStorageFolder = crawlStorageFolder;</span>
<span class="nc" id="L199">  }</span>

  public boolean isResumableCrawling() {
<span class="nc" id="L202">    return resumableCrawling;</span>
  }

  /**
   * If this feature is enabled, you would be able to resume a previously
   * stopped/crashed crawl. However, it makes crawling slightly slower
   *
   * @param resumableCrawling Should crawling be resumable between runs ?
   */
  public void setResumableCrawling(boolean resumableCrawling) {
<span class="nc" id="L212">    this.resumableCrawling = resumableCrawling;</span>
<span class="nc" id="L213">  }</span>

  public int getMaxDepthOfCrawling() {
<span class="nc" id="L216">    return maxDepthOfCrawling;</span>
  }

  /**
   * Maximum depth of crawling For unlimited depth this parameter should be set to -1
   *
   * @param maxDepthOfCrawling Depth of crawling (all links on current page = depth of 1)
   */
  public void setMaxDepthOfCrawling(int maxDepthOfCrawling) {
<span class="nc" id="L225">    this.maxDepthOfCrawling = maxDepthOfCrawling;</span>
<span class="nc" id="L226">  }</span>

  public int getMaxPagesToFetch() {
<span class="nc" id="L229">    return maxPagesToFetch;</span>
  }

  /**
   * Maximum number of pages to fetch For unlimited number of pages, this parameter should be set to -1
   *
   * @param maxPagesToFetch How many pages to fetch from all threads together ?
   */
  public void setMaxPagesToFetch(int maxPagesToFetch) {
<span class="nc" id="L238">    this.maxPagesToFetch = maxPagesToFetch;</span>
<span class="nc" id="L239">  }</span>

  /**
   *
   * @return userAgentString
   */
  public String getUserAgentString() {
<span class="nc" id="L246">    return userAgentString;</span>
  }

  /**
   * user-agent string that is used for representing your crawler to web
   * servers. See http://en.wikipedia.org/wiki/User_agent for more details
   *
   * @param userAgentString Custom userAgent string to use as your crawler's identifier
   */
  public void setUserAgentString(String userAgentString) {
<span class="nc" id="L256">    this.userAgentString = userAgentString;</span>
<span class="nc" id="L257">  }</span>

  /**
   * Return a copy of the default header collection.
   */
  public Collection&lt;BasicHeader&gt; getDefaultHeaders() {
<span class="nc" id="L263">    return new HashSet&lt;&gt;(defaultHeaders);</span>
  }

  /**
   * Set the default header collection (creating copies of the provided headers).
   */
  public void setDefaultHeaders(Collection&lt;? extends Header&gt; defaultHeaders) {
<span class="nc" id="L270">    Collection&lt;BasicHeader&gt; copiedHeaders = new HashSet&lt;&gt;();</span>
<span class="nc bnc" id="L271" title="All 2 branches missed.">    for (Header header : defaultHeaders) {</span>
<span class="nc" id="L272">      copiedHeaders.add(new BasicHeader(header.getName(), header.getValue()));</span>
<span class="nc" id="L273">    }</span>
<span class="nc" id="L274">    this.defaultHeaders = copiedHeaders;</span>
<span class="nc" id="L275">  }</span>

  public int getPolitenessDelay() {
<span class="nc" id="L278">    return politenessDelay;</span>
  }

  /**
   * Politeness delay in milliseconds (delay between sending two requests to
   * the same host).
   *
   * @param politenessDelay
   *            the delay in milliseconds.
   */
  public void setPolitenessDelay(int politenessDelay) {
<span class="nc" id="L289">    this.politenessDelay = politenessDelay;</span>
<span class="nc" id="L290">  }</span>

  public boolean isIncludeHttpsPages() {
<span class="nc" id="L293">    return includeHttpsPages;</span>
  }

  /**
   * @param includeHttpsPages Should we crawl https pages?
   */
  public void setIncludeHttpsPages(boolean includeHttpsPages) {
<span class="nc" id="L300">    this.includeHttpsPages = includeHttpsPages;</span>
<span class="nc" id="L301">  }</span>

  public boolean isIncludeBinaryContentInCrawling() {
<span class="nc" id="L304">    return includeBinaryContentInCrawling;</span>
  }

  /**
   *
   * @param includeBinaryContentInCrawling Should we fetch binary content such as images, audio, ...?
   */
  public void setIncludeBinaryContentInCrawling(boolean includeBinaryContentInCrawling) {
<span class="nc" id="L312">    this.includeBinaryContentInCrawling = includeBinaryContentInCrawling;</span>
<span class="nc" id="L313">  }</span>
  
  public boolean isProcessBinaryContentInCrawling() {
<span class="nc" id="L316">    return processBinaryContentInCrawling;</span>
  }

  /**
   * Should we process binary content such as images, audio, ... using TIKA?
   */
  public void setProcessBinaryContentInCrawling(boolean processBinaryContentInCrawling) {
<span class="nc" id="L323">    this.processBinaryContentInCrawling = processBinaryContentInCrawling;</span>
<span class="nc" id="L324">  }  </span>

  public int getMaxConnectionsPerHost() {
<span class="nc" id="L327">    return maxConnectionsPerHost;</span>
  }

  /**
   * @param maxConnectionsPerHost Maximum Connections per host
   */
  public void setMaxConnectionsPerHost(int maxConnectionsPerHost) {
<span class="nc" id="L334">    this.maxConnectionsPerHost = maxConnectionsPerHost;</span>
<span class="nc" id="L335">  }</span>

  public int getMaxTotalConnections() {
<span class="nc" id="L338">    return maxTotalConnections;</span>
  }

  /**
   * @param maxTotalConnections Maximum total connections
   */
  public void setMaxTotalConnections(int maxTotalConnections) {
<span class="nc" id="L345">    this.maxTotalConnections = maxTotalConnections;</span>
<span class="nc" id="L346">  }</span>

  public int getSocketTimeout() {
<span class="nc" id="L349">    return socketTimeout;</span>
  }

  /**
   * @param socketTimeout Socket timeout in milliseconds
   */
  public void setSocketTimeout(int socketTimeout) {
<span class="nc" id="L356">    this.socketTimeout = socketTimeout;</span>
<span class="nc" id="L357">  }</span>

  public int getConnectionTimeout() {
<span class="nc" id="L360">    return connectionTimeout;</span>
  }

  /**
   * @param connectionTimeout Connection timeout in milliseconds
   */
  public void setConnectionTimeout(int connectionTimeout) {
<span class="nc" id="L367">    this.connectionTimeout = connectionTimeout;</span>
<span class="nc" id="L368">  }</span>

  public int getMaxOutgoingLinksToFollow() {
<span class="nc" id="L371">    return maxOutgoingLinksToFollow;</span>
  }

  /**
   * @param maxOutgoingLinksToFollow Max number of outgoing links which are processed from a page
   */
  public void setMaxOutgoingLinksToFollow(int maxOutgoingLinksToFollow) {
<span class="nc" id="L378">    this.maxOutgoingLinksToFollow = maxOutgoingLinksToFollow;</span>
<span class="nc" id="L379">  }</span>

  public int getMaxDownloadSize() {
<span class="nc" id="L382">    return maxDownloadSize;</span>
  }

  /**
   * @param maxDownloadSize Max allowed size of a page. Pages larger than this size will not be fetched.
   */
  public void setMaxDownloadSize(int maxDownloadSize) {
<span class="nc" id="L389">    this.maxDownloadSize = maxDownloadSize;</span>
<span class="nc" id="L390">  }</span>

  public boolean isFollowRedirects() {
<span class="nc" id="L393">    return followRedirects;</span>
  }

  /**
   * @param followRedirects Should we follow redirects?
   */
  public void setFollowRedirects(boolean followRedirects) {
<span class="nc" id="L400">    this.followRedirects = followRedirects;</span>
<span class="nc" id="L401">  }</span>
  
  public boolean isShutdownOnEmptyQueue() {
<span class="nc" id="L404">      return shutdownOnEmptyQueue;</span>
  }
  
  /**
   * Should the crawler stop running when the queue is empty?
   */
  public void setShutdownOnEmptyQueue(boolean shutdown) {
<span class="nc" id="L411">      shutdownOnEmptyQueue = shutdown;</span>
<span class="nc" id="L412">  }</span>

  public boolean isOnlineTldListUpdate() {
<span class="nc" id="L415">    return onlineTldListUpdate;</span>
  }

  /**
   * Should the TLD list be updated automatically on each run? Alternatively,
   * it can be loaded from the embedded tld-names.txt resource file that was
   * obtained from https://publicsuffix.org/list/effective_tld_names.dat
   */
  public void setOnlineTldListUpdate(boolean online) {
<span class="nc" id="L424">    onlineTldListUpdate = online;</span>
<span class="nc" id="L425">  }</span>

  public String getProxyHost() {
<span class="nc" id="L428">    return proxyHost;</span>
  }

  /**
   * @param proxyHost If crawler should run behind a proxy, this parameter can be used for specifying the proxy host.
   */
  public void setProxyHost(String proxyHost) {
<span class="nc" id="L435">    this.proxyHost = proxyHost;</span>
<span class="nc" id="L436">  }</span>

  public int getProxyPort() {
<span class="nc" id="L439">    return proxyPort;</span>
  }

  /**
   * @param proxyPort If crawler should run behind a proxy, this parameter can be used for specifying the proxy port.
   */
  public void setProxyPort(int proxyPort) {
<span class="nc" id="L446">    this.proxyPort = proxyPort;</span>
<span class="nc" id="L447">  }</span>

  public String getProxyUsername() {
<span class="nc" id="L450">    return proxyUsername;</span>
  }

  /**
   * @param proxyUsername
   *        If crawler should run behind a proxy and user/pass is needed for
   *        authentication in proxy, this parameter can be used for specifying the username.
   */
  public void setProxyUsername(String proxyUsername) {
<span class="nc" id="L459">    this.proxyUsername = proxyUsername;</span>
<span class="nc" id="L460">  }</span>

  public String getProxyPassword() {
<span class="nc" id="L463">    return proxyPassword;</span>
  }

  /**
   * If crawler should run behind a proxy and user/pass is needed for
   * authentication in proxy, this parameter can be used for specifying the password.
   *
   * @param proxyPassword String Password
   */
  public void setProxyPassword(String proxyPassword) {
<span class="nc" id="L473">    this.proxyPassword = proxyPassword;</span>
<span class="nc" id="L474">  }</span>

  /**
   * @return the authentications Information
   */
  public List&lt;AuthInfo&gt; getAuthInfos() {
<span class="nc" id="L480">    return authInfos;</span>
  }

  public void addAuthInfo(AuthInfo authInfo) {
<span class="nc bnc" id="L484" title="All 2 branches missed.">    if (this.authInfos == null) {</span>
<span class="nc" id="L485">      this.authInfos = new ArrayList&lt;AuthInfo&gt;();</span>
    }

<span class="nc" id="L488">    this.authInfos.add(authInfo);</span>
<span class="nc" id="L489">  }</span>

  /**
   * @param authInfos authenticationInformations to set
   */
  public void setAuthInfos(List&lt;AuthInfo&gt; authInfos) {
<span class="nc" id="L495">    this.authInfos = authInfos;</span>
<span class="nc" id="L496">  }</span>

  @Override
  public String toString() {
<span class="nc" id="L500">    StringBuilder sb = new StringBuilder();</span>
<span class="nc" id="L501">    sb.append(&quot;Crawl storage folder: &quot; + getCrawlStorageFolder() + &quot;\n&quot;);</span>
<span class="nc" id="L502">    sb.append(&quot;Resumable crawling: &quot; + isResumableCrawling() + &quot;\n&quot;);</span>
<span class="nc" id="L503">    sb.append(&quot;Max depth of crawl: &quot; + getMaxDepthOfCrawling() + &quot;\n&quot;);</span>
<span class="nc" id="L504">    sb.append(&quot;Max pages to fetch: &quot; + getMaxPagesToFetch() + &quot;\n&quot;);</span>
<span class="nc" id="L505">    sb.append(&quot;User agent string: &quot; + getUserAgentString() + &quot;\n&quot;);</span>
<span class="nc" id="L506">    sb.append(&quot;Include https pages: &quot; + isIncludeHttpsPages() + &quot;\n&quot;);</span>
<span class="nc" id="L507">    sb.append(&quot;Include binary content: &quot; + isIncludeBinaryContentInCrawling() + &quot;\n&quot;);</span>
<span class="nc" id="L508">    sb.append(&quot;Max connections per host: &quot; + getMaxConnectionsPerHost() + &quot;\n&quot;);</span>
<span class="nc" id="L509">    sb.append(&quot;Max total connections: &quot; + getMaxTotalConnections() + &quot;\n&quot;);</span>
<span class="nc" id="L510">    sb.append(&quot;Socket timeout: &quot; + getSocketTimeout() + &quot;\n&quot;);</span>
<span class="nc" id="L511">    sb.append(&quot;Max total connections: &quot; + getMaxTotalConnections() + &quot;\n&quot;);</span>
<span class="nc" id="L512">    sb.append(&quot;Max outgoing links to follow: &quot; + getMaxOutgoingLinksToFollow() + &quot;\n&quot;);</span>
<span class="nc" id="L513">    sb.append(&quot;Max download size: &quot; + getMaxDownloadSize() + &quot;\n&quot;);</span>
<span class="nc" id="L514">    sb.append(&quot;Should follow redirects?: &quot; + isFollowRedirects() + &quot;\n&quot;);</span>
<span class="nc" id="L515">    sb.append(&quot;Proxy host: &quot; + getProxyHost() + &quot;\n&quot;);</span>
<span class="nc" id="L516">    sb.append(&quot;Proxy port: &quot; + getProxyPort() + &quot;\n&quot;);</span>
<span class="nc" id="L517">    sb.append(&quot;Proxy username: &quot; + getProxyUsername() + &quot;\n&quot;);</span>
<span class="nc" id="L518">    sb.append(&quot;Proxy password: &quot; + getProxyPassword() + &quot;\n&quot;);</span>
<span class="nc" id="L519">    return sb.toString();</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.eclemma.org/jacoco">JaCoCo</a> 0.7.6.201602180812</span></div></body></html>